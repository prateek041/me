---
title: "Series Introduction"
description: "Training Language Models, Zero to Hero"
date: January 25, 2025
---

Below is the structure we are going to follow

- **Machine Learning Fundamentals**: Supervised learning, loss functions, optimization.
- **Mathematics Primer**: Derivatives, chain rule, minima/maxima.
- **Deep Learning Fundamentals & Backpropagation**: Neural networks (MLPs), activation
  functions, gradient descent.
- **Tokenization & Embeddings**: Tokenization (BPE), word vectors, context windows.
- **Language Modeling**: From Bigrams to Transformers: Bigram → RNNs → LSTMs →
  Transformers (motivation + intuition).
- **Transformer Architecture**: Self-attention, positional encoding, scalability.
- **Training Dynamics**: Optimizers, regularization, hyperparameters.
- **Capstone**: Train a Tiny LLM:Build a character-level GPT or nano Transformer.

---
